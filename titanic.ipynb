{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Import appropriate packages and set analysis options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot') \n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "from sklearn import linear_model, preprocessing\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "#from sklearn.preprocessing importcross_val_score OneHotEncoder\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from math import sqrt\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Setting save_predicted_data to True\n",
    "# will cause the notebook to save data\n",
    "# in one of the last notebook cells.\n",
    "# The data will be saved to the path\n",
    "# specified by MY_PREDICTED_DATA_PATH.\n",
    "save_predicted_data = False\n",
    "\n",
    "# As expected, setting print_all_output\n",
    "# to True will cause each evaluation in a\n",
    "# a cell to be displayed. This has the\n",
    "# unfortunate side-effect of preventing the\n",
    "# ';' operator from silencing output.\n",
    "# If this boolean is set to False, then\n",
    "# only the last item in each cell may\n",
    "# be output.\n",
    "print_all_output = False\n",
    "InteractiveShell.ast_node_interactivity = 'all' if print_all_output else 'last_expr'\n",
    "\n",
    "# Setting engineer_features to True will\n",
    "# enable the creation of new data/features\n",
    "# from the original data. This makes it\n",
    "# simpler to include/exclude this extra\n",
    "# data and determine whether it helps\n",
    "# improve the models.\n",
    "# WARNING: if set to True, then these same\n",
    "# engineered features must be provided/added\n",
    "# to the test data so that the prediction model\n",
    "# has the same number of input features for both\n",
    "# the training and testing data.\n",
    "engineer_features = False\n",
    "\n",
    "# Setting one_hot_encoding to True will cause\n",
    "# categorical features not only to be encoded\n",
    "# but to be one-hot encoded. This transforms\n",
    "# all categorical labels into individual\n",
    "# columns to prevent spurious effects related\n",
    "# to sequential int encodings.\n",
    "# WARNING: if set to True, the training and test\n",
    "# data will be encoded with a different number of\n",
    "# categorical labels / columns, so the training\n",
    "# model would not be applicable to the test data.\n",
    "# Currently, this notebook does not support\n",
    "# one-hot encoding both the training and testing\n",
    "# data sets.\n",
    "one_hot_encoding = False\n",
    "\n",
    "# Setting randomize_seeding to True will\n",
    "# randomize various operations throughout\n",
    "# the notebook. Setting it to False will\n",
    "# cause the seed to remain fixed to some\n",
    "# specified value such that the notebook\n",
    "# can be reran with the same randomized\n",
    "# variables (see MAGIC_SEED below).\n",
    "randomize_seeding = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define convenient variables and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These paths indicate from where the training,\n",
    "# test, and prediction data will be loaded/saved.\n",
    "TRAINING_DATA_PATH = \"./data/train.csv\"\n",
    "TEST_DATA_PATH = \"./data/test.csv\"\n",
    "MY_PREDICTED_DATA_PATH = \"./data/my_predicted_survivors.csv\"\n",
    "\n",
    "# The MAGIC_SEED optionally specifies a fixed\n",
    "# random state/seed so that the notebook can be\n",
    "# reran with the same randomized variables (see\n",
    "# randomize_seeding above).\n",
    "MAGIC_SEED = 1776\n",
    "if (randomize_seeding):\n",
    "    MAGIC_SEED = np.random.seed()\n",
    "\n",
    "# The training data provided with this data\n",
    "# set will be split into two subsets so that\n",
    "# models can be trained on the first and tested\n",
    "# on the second. TRAINING_DATA_TEST_SIZE\n",
    "# indicates the proportion of the training\n",
    "# data that will be used as test data for\n",
    "# model evaluation and should be in the\n",
    "# range [0.0, 1.0].\n",
    "TRAINING_DATA_TEST_SIZE = 0.10\n",
    "\n",
    "def load_data(path, index_column):\n",
    "    \"\"\"\n",
    "    Load the file at 'path' into a Pandas\n",
    "    DataFrame.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path, header=0, index_col=index_column)\n",
    "    print(\"Loaded data dimensions: \", df.shape[0], \"rows, \", df.shape[1], \"columns\")\n",
    "    return df\n",
    "\n",
    "def print_nan(nan_cols_counts, col_type):\n",
    "    \"\"\"\n",
    "    Print each element of the list which should contain\n",
    "    a DataFrame feature name and an int number of times\n",
    "    the feature contains an NaN value.\n",
    "    \"\"\"\n",
    "    print(\"\\n\", len(nan_cols_counts), \" \", col_type, \"-type columns with NaN values.\", sep='')\n",
    "    if(len(nan_cols_counts) > 0):\n",
    "        print(\"    {:<16}{}\".format(\"Feature\", \"NaN Count\"))\n",
    "        print(\"%s\" % \"    ---------------------\")\n",
    "    for index, element in enumerate(nan_cols_counts):\n",
    "        print(\"{:>2}. {:<16}{}\".format(index+1, element[0], element[1]))\n",
    "        \n",
    "def gather_nan(df, col_type, print_if_nan = True):\n",
    "    \"\"\"\n",
    "    Find all DataFrame columns of type 'col_type'\n",
    "    which contain NaN values.\n",
    "    \"\"\"\n",
    "    if (col_type == \"int\"):\n",
    "        columns = df.select_dtypes(include=['int']).columns\n",
    "    elif (col_type == \"float\"):\n",
    "        columns = df.select_dtypes(include=['float']).columns\n",
    "    else:\n",
    "        columns = df.select_dtypes(include=['object']).columns\n",
    "    nan_cols_counts = []\n",
    "    for col in np.sort(columns):\n",
    "        num_nan = np.sum(df[col].isnull())\n",
    "        if (num_nan > 0):\n",
    "            nan_cols_counts.append((col, num_nan))\n",
    "    if (print_nan):\n",
    "        print_nan(nan_cols_counts, col_type)\n",
    "    return nan_cols_counts\n",
    "            \n",
    "def replace_with_normal(df, col, seed = np.random.seed()):\n",
    "    \"\"\"\n",
    "    Replace NaN values in a DataFrame column with\n",
    "    values chosen from a normal distribution with\n",
    "    a mean and standard deviation equal to the\n",
    "    that of the non-NaN data.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    df_dropped = df[col].dropna()\n",
    "    mu = np.mean(df_dropped)\n",
    "    sigma = np.std(df_dropped)\n",
    "    null_rows = df[col].isnull()\n",
    "    num_nan = np.sum(null_rows)\n",
    "    rand_vals = np.random.normal(mu, sigma, num_nan)\n",
    "    df.loc[null_rows, col] = rand_vals\n",
    "    \n",
    "def most_common_label(df, col):\n",
    "    \"\"\"\n",
    "    Determine the most fequent label for\n",
    "    categorical data.\n",
    "    \"\"\"\n",
    "    most_common_appearances = 0\n",
    "    most_common_label = \"\"\n",
    "    for label in df[col].unique():\n",
    "        num_appearances = np.sum(df[col] == label)\n",
    "        if (num_appearances > most_common_appearances):\n",
    "            most_common_appearances = num_appearances\n",
    "            most_common_label = label\n",
    "    return most_common_label\n",
    "\n",
    "def evaluate_model(x_train, y_train, model):\n",
    "    \"\"\"\n",
    "    Given an input model and training data,\n",
    "    split the data into training/testing subsets\n",
    "    and use this to produce a fit and predictions.\n",
    "    Indicate the goodness of the fit and plot\n",
    "    the results.\n",
    "    \"\"\"\n",
    "    # Split the training data into two subsets.\n",
    "    # Then, train the model on the target data\n",
    "    # and use it to predict home prices.\n",
    "    x_train1, x_train2, y_train1, y_train2 = train_test_split(\n",
    "        x_train, y_train,\n",
    "        test_size=TRAINING_DATA_TEST_SIZE,\n",
    "        random_state=MAGIC_SEED)\n",
    "    model.fit(x_train1, y_train1)\n",
    "    y_train2_pred = model.predict(x_train2)\n",
    "    \n",
    "    # Evaluate the model & predictions by viewing\n",
    "    # the cross-validation score, error, and\n",
    "    # variance (where a variance of 1 indicates\n",
    "    # a perfect prediction) and plotting the results.\n",
    "    print(\"When using %0.1f%% of the training data to perform the\"\n",
    "        \" fit and %0.1f%% of the training data to make the prediction,\"\n",
    "        \" the model performed according to the following:\"\n",
    "        % (100.0*(1.0-TRAINING_DATA_TEST_SIZE), 100.0*TRAINING_DATA_TEST_SIZE))\n",
    "    print(cross_val_score(model, x_train1, y_train1, cv=5))\n",
    "    print(\"RMS Error: %.3f\"\n",
    "        % sqrt(mean_squared_error(y_train2, y_train2_pred)))\n",
    "    print('Variance score: %.3f' % r2_score(y_train2, y_train2_pred))\n",
    "#    plt.hist(y_train2, bins=2, color='red')\n",
    "#    plt.hist(y_train2_pred, bins=2, histtype='step', color='black')\n",
    "    x = np.arange(4)\n",
    "    y = [y_train2.tolist().count(1), y_train2_pred.tolist().count(1),\n",
    "         y_train2.tolist().count(0), y_train2_pred.tolist().count(0)]\n",
    "    colors = (\"green\", \"cyan\", \"red\", \"magenta\")\n",
    "    plt.bar(x, y, color=colors)\n",
    "    plt.xticks(x, (\"True Survival\", \"Pred. Survival\",\n",
    "        \"True Decease\", \"Pred. Decease\"))\n",
    "    plt.show()\n",
    "    \n",
    "    # Finalize the model by fitting it to the entire data set.\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "def most_important_features(df, feature_importances, num_features):\n",
    "    \"\"\"\n",
    "    Select and pritn out the N most important features used\n",
    "    in the model to make predictions.\n",
    "    \"\"\"\n",
    "    importances = model.feature_importances_\n",
    "    sorted_indices = np.argsort(importances)[::-1].tolist()\n",
    "    top_n_indices = sorted_indices[:5]\n",
    "    print(\"The %d most important features for this model:\" % num_features)\n",
    "    for ii, index in enumerate(top_n_indices):\n",
    "        print(\"%d. %s\" % (ii+1, df.columns[index]))\n",
    "\n",
    "def encode(df):\n",
    "    \"\"\"\n",
    "    Convert categorical labels to ints.\n",
    "    \"\"\"\n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    encoded_label_groups = []\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        label_encoder.fit(df[col].unique());\n",
    "        encoded_label_groups.append(label_encoder.classes_.tolist())\n",
    "        df.loc[:, col] = label_encoder.transform(df[col]);\n",
    "    return encoded_label_groups\n",
    "\n",
    "# The following function was originally written for debugging\n",
    "# purposes but was retained in case it was useful in the future.\n",
    "# For an arbitrary feature index, the function prints details\n",
    "# about that particular feature's encoding and shows that the\n",
    "# one_hot_cols columns properly partition/encode the int values\n",
    "# in the feature's column.\n",
    "def test_one_hot_encoding(df, features_and_encoded_labels, one_hot_cols, feature_index):\n",
    "    \"\"\"\n",
    "    Print a few elements of an encoded column and compare\n",
    "    the data to the one-hot encoded columns.\n",
    "    \"\"\"\n",
    "    categorical_features = [pair[0] for pair in features_and_encoded_labels]\n",
    "    encoded_label_groups = [label for pair in features_and_encoded_labels for label in pair[1]]\n",
    "    feature = categorical_features[feature_index]\n",
    "    label_group_starting_index = sum(len(labels) for labels in encoded_label_groups[0:feature_index])\n",
    "    num_group_labels = len(encoded_label_groups[feature_index])\n",
    "    feature_label_indices = range(label_group_starting_index, label_group_starting_index+num_group_labels, 1)\n",
    "    print(\"# categorical_features: %d\\n# encoded_label_groups: %d\"\n",
    "        % (len(categorical_features), len(encoded_label_groups)))\n",
    "    print(\"(feature_index, feature, label_group_starting_index, # group labels): (%d, %s, %d, %d)\"\n",
    "        % (feature_index, feature, label_group_starting_index, num_group_labels))\n",
    "    print(df[feature][0:10])\n",
    "    for ii in feature_label_indices:\n",
    "        print(one_hot_cols[ii][0:10])\n",
    "\n",
    "def one_hot_encode(df, features_and_encoded_labels):\n",
    "    \"\"\"\n",
    "    Perform one-hot encoding on already-encoded data to\n",
    "    transform each feature label into its own column. This\n",
    "    helps prevent categorical variable integer mappings\n",
    "    from indirectly influencing models/fits.\n",
    "    Assumptions:\n",
    "    1. The categorical features of the input DataFrame df \n",
    "    are already encoded.\n",
    "    \"\"\"\n",
    "    # Gather the already-encoded data.\n",
    "    categorical_features = [pair[0] for pair in features_and_encoded_labels]\n",
    "    categorical_values = [df[label].tolist() for label in categorical_features]\n",
    "    categorical_matrix_transpose = np.array(categorical_values).T.tolist()\n",
    "    \n",
    "    # One-hot encode it.\n",
    "    one_hot_encoder = OneHotEncoder(dtype=int, sparse=False)\n",
    "    one_hot_encoder.fit(categorical_matrix_transpose)\n",
    "    print(\"One-hot encoding %d unique categorical features and %d unique categorical labels\"\n",
    "        % (len(one_hot_encoder.n_values_), sum(one_hot_encoder.n_values_)))\n",
    "    one_hot_cols = one_hot_encoder.transform(categorical_matrix_transpose).T.tolist()\n",
    "    \n",
    "    # Test the encoding.\n",
    "    if (False):\n",
    "        dummy_test_index = 0\n",
    "        test_one_hot_encoding(df, features_and_encoded_labels, one_hot_cols, dummy_test_index)\n",
    "    \n",
    "    # Remove the original categorical columns and \n",
    "    # add the new one_hot_cols to DataFrame.\n",
    "    df.drop(categorical_features, axis='columns', inplace=True)\n",
    "    for index, item in enumerate(features_and_encoded_labels):\n",
    "        feature = item[0]\n",
    "        labels = item[1]\n",
    "        for label in labels:\n",
    "            df[feature + \":\" + label] = one_hot_cols[index]\n",
    "            \n",
    "    return one_hot_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and preview the passenger training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data(TRAINING_DATA_PATH, 'PassengerId')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean and process the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition data into features (X-data) and\n",
    "# targets (Y-data).\n",
    "x_train = df.iloc[:,1:]\n",
    "y_train = df.iloc[:,0]\n",
    "\n",
    "# Determine which features have missing values.\n",
    "nan_int_cols = gather_nan(x_train, \"int\")\n",
    "nan_float_cols = gather_nan(x_train, \"float\")\n",
    "nan_string_cols = gather_nan(x_train, \"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, determine how the Age data is distributed.\n",
    "x_age = x_train['Age'].dropna().tolist()\n",
    "mu_age = np.mean(x_age)\n",
    "sigma_age = np.std(x_age)\n",
    "\n",
    "# Generate age samples using the age data's\n",
    "# average and standard deviation. Graphically\n",
    "# compare the real and fabricated data (normalized\n",
    "# for easy comparison).\n",
    "x_age_generated = np.random.normal(mu_age, sigma_age, len(x_age))\n",
    "\n",
    "# Set all ages below zero to zero.\n",
    "x_age_generated[(x_age_generated <= 0)] = 0\n",
    "plt.hist(x_age, bins=20, color='red', normed=0)\n",
    "plt.hist(x_age_generated, bins=20, histtype='step', color='black', normed=0)\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend((\"True Age Data\", \"Generated Age Data\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visually, the data distributions appear somewhat resemblant. Directly test whether the age data is normal by testing the similarity between the true and fabricated data distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test whether these data sets have similar\n",
    "# distributions using the Kolmogorov-Smirnov\n",
    "# test. A large p-value (e.g. > 0.05) indicates\n",
    "# that the distributions are similar enough to\n",
    "# warrant considering them the same.\n",
    "stats.ks_2samp(x_age, x_age_generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For most randomized tests, the Klmogorov-Smirnov test tends to indicate that the age data is not normally distributed. However, as a first approximation, assume the age data is normally distributed and replace all NaN values with values randomly selected from a normal distribution with the same mean and standard deviation as the non-NaN value data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "replace_with_normal(x_train, 'Age', MAGIC_SEED)\n",
    "\n",
    "# Set all ages below zero to zero.\n",
    "x_train.loc[x_train['Age'] < 0, 'Age'] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'Cabin' feature is very sparse, so drop\n",
    "# this feature since it can provide only very\n",
    "# limited information.\n",
    "print(\"Out of %d instances, %d have missing Cabin information; dropping this feature.\"\n",
    "      % (df.shape[0], sum(x_train['Cabin'].isna())))\n",
    "x_train.drop('Cabin', axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# It seems unlikely that the passenger's name,\n",
    "# ticket number, or port of embarkation would\n",
    "# correlate with their survival. Drop these features.\n",
    "x_train.drop('Name', axis='columns', inplace=True)\n",
    "x_train.drop('Ticket', axis='columns', inplace=True)\n",
    "x_train.drop('Embarked', axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Retrieve the categorical column labels.\n",
    "categorical_features = x_train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Encode all categorical features.\n",
    "encoded_label_groups = encode(x_train)\n",
    "features_and_encoded_labels = list(zip(categorical_features, encoded_label_groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this point, the DataFrame should be free of\n",
    "# missing values.\n",
    "nan_int_cols = gather_nan(x_train, \"int\")\n",
    "nan_float_cols = gather_nan(x_train, \"float\")\n",
    "nan_string_cols = gather_nan(x_train, \"string\")\n",
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the survival rate as a function of other features - class\n",
    "# (\"Pclass\"), age, and fare - along with a 98% confidence interval\n",
    "# to get an idea of how various features influence survival.\n",
    "\n",
    "sns.regplot(x=x_train[\"Pclass\"], y=y_train, n_boot=100,\n",
    "    logistic=True, ci=98, color='red', x_estimator=np.mean)\n",
    "plt.show()\n",
    "\n",
    "sns.regplot(x=x_train[\"Age\"], y=y_train, n_boot=100,\n",
    "    logistic=True, ci=98, color='green')\n",
    "plt.show()\n",
    "\n",
    "sns.regplot(x=x_train[\"Fare\"], y=y_train, n_boot=100,\n",
    "    logistic=True, ci=98, color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform hyperparameter tuning for a logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimator = linear_model.LogisticRegression(random_state=MAGIC_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method #1: Grid Search w/ Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_grid ={\n",
    "    \"penalty\": ['l1', 'l2'],\n",
    "    \"tol\": [1e-4, 1e-5, 1e-6],\n",
    "    \"C\": [0.25, 0.5, 0.75, 1.0],\n",
    "    \"fit_intercept\": [True, False]\n",
    "}\n",
    "model = GridSearchCV(estimator, cv=5, return_train_score=True,\n",
    "    param_grid=parameter_grid)\n",
    "evaluate_model(x_train, y_train, model)\n",
    "print(\"The best parameters are:\")\n",
    "print(model.best_params_)\n",
    "print(\"with a score of %.3f\" % model.best_score_)\n",
    "\n",
    "# Save the model for later usage.\n",
    "prediction_model = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method #2: Randomized Search w/ Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 25\n",
    "parameter_distributions = {\n",
    "    \"penalty\": ['l1', 'l2'],\n",
    "    \"tol\": np.arange(1e-6, 1e-4, 1e-6),\n",
    "    \"C\": np.arange(0.1, 1.0, 0.01),\n",
    "    \"fit_intercept\": [True, False]\n",
    "} \n",
    "    \n",
    "model = RandomizedSearchCV(estimator, cv=5, return_train_score=True,\n",
    "    param_distributions=parameter_distributions, n_iter=num_iterations, \n",
    "    random_state=MAGIC_SEED)\n",
    "evaluate_model(x_train, y_train, model)\n",
    "print(\"The best parameters are:\")\n",
    "print(model.best_params_)\n",
    "print(\"with a score of %.3f\" % model.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The grid and randomized searched seem to return similar results, so arbitrarily use the grid search parameters to make predictions for the passenger data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
